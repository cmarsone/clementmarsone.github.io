---
layout: post
title: Convex Optimization
---

#### Convex set: $U \subseteq R^n$

$\forall u, u^{\prime} \in U, \exists \epsilon \in [0,1] $ such that $\underbrace{\epsilon u +(1 -\epsilon)u^{\prime}}_ {\text{convex comb.}} \in U$

#### Convex hull: $\operatorname{conv} U$ the smallest convex set that contains $U$

$\operatorname{conv} U = \lbrace \epsilon u +(1 -\epsilon)u^\prime \mid u, u^\prime \in U, \epsilon \in [0, 1] \rbrace $
For instance, simplex $\operatorname{conv} E(k) = \Delta(k)$

#### Convex function $f:U \to \mathbb{R}$

1. $U$ is a convex set
2. $\forall u, u^\prime \in U, Îµ \in [0, 1], f( \underbrace{\epsilon u +(1 -\epsilon)u^\prime}_ {\text{dom. needs to be conv.}}) \leq \epsilon f(u) +(1 -\epsilon)f(u^\prime)$ or $\forall u \in U, \nabla^2f(u)$ (Hessian) is a positive semi-definite matrix ($\langle u,\nabla^2f(u) \rangle$ 

In other words, any point which is a convex combination of points in $U$ must also be in $U$. Note that the domain of the function is required to be convex so that the left-hand side is well defined. A function $f$ is concave if and only if $âˆ’f$ is convex.

#### (Epigraph) Domain of a function $\operatorname{dom}(f) = \lbrace x \in \mathbb{R}^n \mid f(x) < +\infty \rbrace$

#### Proper function $f:U \to \mathbb{R} \cup \lbrace - \infty, + \infty \rbrace$ 

If it does not attain the value $-\infty$ and $\operatorname{dom}(f) \neq \0$ i.e.
1. $\forall u \in U, f (u) \neq -\infty$
2. $\exists u \in U, f (u) \neq +\infty$

#### Closed epigraph $\iff$ lower semi-continuous $\iff$ closed $\alpha$-level sets

A function is *closed* $\iff$ its *epigraph is closed*

$\iff$ A function $f : \mathbb{R}^n \to [âˆ’\infty, +\infty]$ is called *lower semi-continuous* at $x \in \mathbb{R}$ if $\liminf_ {x \to \bar x} f (x) = f (\bar x)$ for any sequence $\lbrace x_ n \rbrace_ {n \geq 1} \subseteq \mathbb{E}$ for which $x_ n \to x$ as $n \to \infty$ or at each point of $\mathbb{R}^n$ in other terms.

$\iff$ The $Î±$-*lower level set* of $f$ is the set: $\operatorname{lev}_ { \leq \alpha} f := \lbrace x \in S: f (x) \leq \alpha \rbrace$

#### Transform a constrained optimization into an unconstrained problem

A function that can take values $âˆ’âˆž$ or $+âˆž$ is called an *extended real-valued function*. That is $f : \mathbb{R}^n â†’ [âˆ’âˆž, +âˆž]$. We can also denote $[âˆ’âˆž, +âˆž]$ by $\mathbb{R}\cup \{ Â±âˆž \}$. The domain of this function is defined by the set dom $(f) = \{x \in \mathbb{R}^n \mid f(x) < +âˆž \}$

$$f(x) = 
\begin{cases} f(x) & \text{if } x \in \text{dom } \\
f = \infty & \text{otherwise} \end{cases}$$

For an arbitrary set $S \subset \mathbb{R}^n$, the *indicator function* of $S$ is defined by the following
extended real-valued function:

$$ \delta_ S (s) =
\begin{cases}
0 & \text{if }s \in S, \\
+âˆž & \text{otherwise.}
\end{cases}$$

### Operations preserving convexity of functions

Weighted sum, Maximum & Linear transformation preserve all the convexity of functions

### Gradient

*Scalar input* $\to$ take the derivative $\to$ compute the linear approximation : 
We can use the chain rule to derive composed functions.

*Vector input* $\to$ take the partial derivative $\to$ compute the gradient : 
We can use the chain rule for vectors by a sum of derivatives.

### Subgradients

Given a function $f : \mathbb{R}^n â†’ \mathbb{R}^n \cup \lbrace âˆž \rbrace$, a subgradient at $u âˆˆ U$ is a vector $g âˆˆ \mathbb{R}^n$ such that:
$âˆ€u\prime âˆˆ \mathbb{R}^n : f (u\prime) â‰¥ f (u) + \langle g, u\prime âˆ’ u \rangle$
The set of *subgradients* at point $u$ is called the *subdifferential* and is denoted $âˆ‚f (u)$.

Properties:
IF $f$ is convex:
- if $f$ is differentiable at $u$, then $\partial f(u) = \lbrace \nabla f(x) \rbrace i.e. the gradient is the single subgradient
- the function $h(u^{\prime})=f(u) + \langleg, u^{\prime}-u \rangle$ is a linear *sub-estimator* of $f$ 
- the *super-gradient* is a similar definition for a concave fucnrion



beck 3.14Theorem 3.14 (nonemptiness and boundedness of the subdifferential set at interior points of the domain). Let f : E â†’ (âˆ’âˆž,âˆž] be a proper convex function, and assume that x Ìƒ âˆˆ int(dom(f)). Then âˆ‚f(x Ìƒ) is nonempty and bounded.


 #### compute subgradients 3.4

strong and weak subgradients

multiplication by a positive scalar 3,35
Theorem 3.35. Let f : E â†’ (âˆ’âˆž, âˆž] be a proper function and let Î± > 0. Then for any x âˆˆ dom(f)
âˆ‚(Î±f)(x) = Î±âˆ‚f(x).
 
summation 3.36
Theorem 3.36. Let f1,f2 : E â†’ (âˆ’âˆž,âˆž] be proper convex functions, and let x âˆˆ dom(f1) âˆ© dom(f2).
(a) The following inclusion holds:
âˆ‚f1(x) + âˆ‚f2(x) âŠ† âˆ‚(f1 + f2)(x).
(b) If x âˆˆ int(dom(f1)) âˆ© int(dom(f2)), then
âˆ‚(f1 + f2)(x) = âˆ‚f1(x) + âˆ‚f2(x).

maimization 3.50
The following result shows how to compute the subdifferential set of a maximum of a finite collection of convex functions.
Theorem 3.50 (max rule of subdifferential calculus). Let f1, f2, . . . , fm : E â†’ (âˆ’âˆž, âˆž] be proper convex functions, and define
f(x) = max{f1(x),f2(x),...,fm(x)}. Let x âˆˆ mi=1 int(dom(fi)). Then

âˆ‚f(x) = conv âˆªiâˆˆI(x)âˆ‚fi(x) , where I(x) = {i âˆˆ {1,2,...,m} : fi(x) = f(x)}.

un constrained optimization pronlem fermat s theorem

#### Fenchel conjugates

Fenchelâ€™s inequality
From the definition of conjugate function, we immediately obtain the inequality f(x) + fâˆ—(y) â‰¥ xT y
for all x, y. This is called Fenchelâ€™s inequality (or Youngâ€™s inequality when f is differentiable).
For example with f(x) = (1/2)xT Qx, where Q âˆˆ Sn++, we obtain the inequality xT y â‰¤ (1/2)xT Qx + (1/2)yT Qâˆ’1y.
Conjugate of the conjugate
The examples above, and the name â€˜conjugateâ€™, suggest that the conjugate of the conjugate of a convex function is the original function. This is the case provided a technical condition holds: if f is convex, and f is closed (i.e., epi f is a closed set; see Â§A.3.3), then fâˆ—âˆ— = f. For example, if domf = Rn, then we have fâˆ—âˆ— = f, i.e., the conjugate of the conjugate of f is f again 

Differentiable functions
The conjugate of a differentiable function f is also called the Legendre transform of f. (To distinguish the general definition from the differentiable case, the term Fenchel conjugate is sometimes used instead of conjugate.)
Suppose f is convex and differentiable, with domf = Rn. Any maximizer xâˆ— of yT xâˆ’f(x) satisfies y = âˆ‡f(xâˆ—), and conversely, if xâˆ— satisfies y = âˆ‡f(xâˆ—), then xâˆ— maximizes yT x âˆ’ f(x). Therefore, if y = âˆ‡f(xâˆ—), we have
fâˆ—(y) = xâˆ—T âˆ‡f(xâˆ—) âˆ’ f(xâˆ—).
This allows us to determine fâˆ—(y) for any y for which we can solve the gradient
Then we have fâˆ—(y) = zT âˆ‡f(z) âˆ’ f(z).




Jensen's inequality
let w =sum_i mu_ i w_i and g in partial f(w), we need w in dom f by previous theorem
by def of subgradient:
forall i f(w_ i) >= f(w) + <g, w_ i -w >
therefore sum _ i mu_ i f_i(w) >= sum mu_ i f(w) + sum mu i <g, w i-w> which is 0

example: is log-partition = log sum exp w_ i convex ? proof by jensen or holder s inequalities
