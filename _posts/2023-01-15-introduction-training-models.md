---
layout: post
title: Training machine learning models
---

### Notations

* $X, Y, \ldots$ random variables
* $x, y, \ldots$ values
* Given a vector $x \in \mathbb{R}^d$ with $d$ is the number of feature/input values, we want to predict an output $y \to f(Y \mid X=x) \in \mathbb{R}$ for regression & binary classification or $ \in \mathbb{R}^k$ for multiclass classification or strutured prediction.
* $\mathbb{P}(X, Y)$ : data distribution
* $\mathbb{P}_ {\theta}(Y \mid X)$ : model distribution over outputs
* $\sum_i \exp(\omega_i)$ : partition
* $\log\sum_i \exp(\omega_i)$ : log-partition
* $\text{conv}(Y)$ : the Convex Hull i. e. the smallest convex set that contains $Y$ 

## General framework : Proof of convergence rates of different optimization techniques

* (input space) $\to$ scoring function $\omega=\sigma_\theta(x)$ $\to$ (score/logit/weight space) $\to$ prediction function $\mu=\hat{y}(\omega)$ $\to$ (output space)
* Training machine learning models is an 'optimization problem'. The aim is to find the 'minimum of a function'. Be careful to distinguish between *min* (a single value), *argmin* (a set of values), *inf* (the lower bound/limit) & conversely for a *max*.
* Actually, we have to define a loss function $\ell:Y\times\mathbb{R}^k \to \mathbb{R}_ {+}$ & try to minimize it over a distribution by finding $\theta=\text{argmin}_ \theta \mathbb{E}_ {\mathbb{P}(X,Y)}(\ell(y, \sigma_ {\theta(x)})$
* 'Monte-Carlo estimation' : $D$ training data, set of i.i.d samples from $\mathbb{P}(X,Y)$ such that $\mathbb{E}_ {\mathbb{P}(X,Y)}(\ell(y, \sigma_\theta(x)) \sim \frac{1}{D} \sum_{(x,y) \in D} \ell(y, \sigma_\theta(x)$
* Then we add in the loss function a regularization term to avoid overfitting $\to$ regularization weights : $L_1$ (LASSO), $L_2$ (Ridge), Elastic-net, $\ldots$. âš  Genrally, do not regularize the intercept ($\neq$ `fit_intercept`)

### Types of problem

#### Regression 

$Y \in \mathbb{R}$, $\text{conv}(Y)=\mathbb{R}$, $\sigma_\theta(x)=a^T x + b = \langle a, x \rangle + b$, $\theta(a, b)$, $a \in \mathbb{R}^d$, $b \in \mathbb{R} \to \hat{y}(\omega) = \omega$
#### Binary classification

$Y = \{ 0, 1 \}$, $\text{conv}(Y)=\[ 0, 1 \]$, $\sigma_\theta(x)=a^T x + b = \langle a, x \rangle + b$, $\theta(a, b)$, $a \in \mathbb{R}^d$, $b \in \mathbb{R} \to \hat{y}(\omega) = \omega$

$$\hat{y}(\omega) = \begin{cases}
1 & \text{if } \omega \ge 0 \\
0 & \text{otherwise}
\end{cases}$$ 

or $\hat{y}(\omega) = \frac{\exp(\omega)}{1 + \exp{\omega}} = \sigma(\omega) \in \] 0, 1 \[$ (sigmoid)

$\mu = \hat{y}(\omega)$, $\mu$ should be interpreted as the parameter of a Bernoulli distribution

$$\begin{cases}
\mathbb{P}_ \theta (Y = 1 \mid X = x) = \mu \\
\mathbb{P}_ \theta (Y = 0 \mid X = x) = 1 - \mu
\end{cases}$$

The following prediction function is an *optimal prediction rule*. It is kown as the *Bayes classifier* :

$$f^* (x) = \mathbb{1} \{ \mu \ge 1/2 \} \in \operatorname{argmax}_{y \in \{ 0, 1\}} P(Y = y \mid X = x)$$ and $\epsilon^* = \mathbb{E} \left[ \min \left( \eta (X), 1 - \eta (X) \right) \right] $

#### Multiclass classification

$Y$ is the set of one-hot vectors of dimension $k$, $\text{conv}(Y)=\Delta(k)$ simplex of dim $k = \{ \mu \in \mathbb{R}^k \mid \sum_ i \mu_ i = 1 \text{ and } \mu_ i \ge 0, \forall i \} $

$\sigma_\theta(x)=a^T x + b = \langle a, x \rangle + b$, $\theta(a, b)$, $a \in \mathbb{R}^{k \times d}$, $b \in \mathbb{R}^k \to \hat{y}(\omega) = \text{argmax}_ {y \in Y} \langle y, \omega \rangle$ or $\hat{y}(\omega) = \text{softmax}(\omega) \iff \mu_ i = \frac{\exp(\omega_ i)}{\sum_ j (\omega_ j)}$

#### Multilabel : structured prediction

Multilabel classification is a machine learning problem where a model is trained to predict multiple output variables for a given input.

### Convex optmization

#### Convex set

A set $U leg R^n$ is convex iff forall u, u^' in U, exists epsilon in 0, 1 s.t. epsilon u

#### Convex Hull

conv U = simplex 

#### Convex function 

*Convex sets* : A set $XâŠ†R^n$ is convex if and only if:
âˆ€x,xâ€² âˆˆX,Îµâˆˆ[0,1]:Îµx+(1âˆ’Îµ)xâ€² âˆˆX,
or, in other words, any point which is a convex combination of points in X must also be in X.

*Convex and concave functions*
LetXâŠ†Rn beaconvexset.Afunctionf:Xâ†’Risconvexifandonlyif:
âˆ€x,xâ€² âˆˆX,Îµâˆˆ[0,1]:f(Îµx+(1âˆ’Îµ)xâ€²)â‰¤Îµf(x)+(1âˆ’Îµ)f(xâ€²)
Note that the domain of the function is required to be convex so that the left-hand side is well defined. A function f is concave if and only if âˆ’f is convex.

Hessian, Positive semi-definitive matrix and its equivalence with convex function
proper function, Closed function iff its epigraph is closed equivlent to semi lower continuity

Extended real value extension => transform a constrained optimization problem into an unconstrained problem

Only at this subsection, we adopt the following rule:
0Â·âˆž = âˆžÂ·0 = 0Â·(âˆ’âˆž) = (âˆ’âˆž)Â·0 = 0.
Î¼2 2 +L âˆ’ Î¼âˆ¥x âˆ’ yâˆ¥2,
and the result follows after some simplifications.

*Extended Real-Valued Functions* : A function that can take values âˆ’âˆž or +âˆž is called an extended real-valued function. That is f : Rn â†’ [âˆ’âˆž, +âˆž]. We can also denote [âˆ’âˆž, +âˆž] by R âˆª {Â±âˆž}. The domain of this function is defined by the set dom(f) = {x âˆˆ Rn | f(x) < +âˆž}.

For an arbitrary set S âŠ‚ Rn, the indicator function of S is defined by the following
extended real-valued function:

A function f : Rn â†’ [âˆ’âˆž, +âˆž] is called *proper *if it does not attain the value âˆ’âˆž and dom(f) Ì¸= âˆ…. This function is called closed if its epigraph is a closed set.
A function f : Rn â†’ [âˆ’âˆž, +âˆž] is called lower *semicontinuous *at x âˆˆ R if f(x) â‰¤ lim inf f(xn)
nâ†’âˆž
for any sequence {xn}âˆžn=1 for which xn â†’ x. Therefore, a function f: Rn â†’ [âˆ’âˆž,+âˆž] is called
lower semicontinous if it is lower semicontinuous at each point of Rn.
Theorem 5.28 Let f : Rn â†’ [âˆ’âˆž, +âˆž]. Then the following conditions are equivalent:
1. f is lower semicontinuous.
2. f is closed.
3. For any Î» âˆˆ R, the Î»-level sets LÎ» of f (see Theorem 5.3) are closed.

Definition 5.29 An extended real-valued function f: Rn â†’ [âˆ’âˆž,+âˆž] is called convex if its epi-
graph is a convex set.
Therefore, we can show that the a proper extended real-valued function is a convex function if and only if it satisfies the condition for usual functions (Definition 5.1) using the rule (10).

### Operations preserving convexity of functions

Weighted sum of functions Beck 2.7 2.16
Maximum sum of functions
linear transformation (convex)

Scalar inout 
derivative:
Linear approximation : 
Chain rule

 Vector input:
partial derivative:
gradient:
chain rule for vector (sum):

### Subgradients

Definition 3.6: Subgradient
Given a function f : X âŠ† Rn â†’ R, a subgradient of f at x âˆˆ X is a vector g âˆˆ Rn such that: âˆ€xâ€²âˆˆX: f(xâ€²)â‰¥f(x)+gâŠ¤(xâ€²âˆ’x)
The set of subgradients at point x is called the subdifferential and is denoted âˆ‚f(x).

Properties:
f is convex 
- if f is differentiable at u then partial f(u) = {nab,a f(x)
- sub estimator h u' 
- super gradient is a similar definition for. oncave fucnrion



beck 3.14Theorem 3.14 (nonemptiness and boundedness of the subdifferential set at interior points of the domain). Let f : E â†’ (âˆ’âˆž,âˆž] be a proper convex function, and assume that x Ìƒ âˆˆ int(dom(f)). Then âˆ‚f(x Ìƒ) is nonempty and bounded.


 #### compute subgradients 3.4

strong and weak subgradients

multiplication by a positive scalar 3,35
Theorem 3.35. Let f : E â†’ (âˆ’âˆž, âˆž] be a proper function and let Î± > 0. Then for any x âˆˆ dom(f)
âˆ‚(Î±f)(x) = Î±âˆ‚f(x).
 
summation 3.36
Theorem 3.36. Let f1,f2 : E â†’ (âˆ’âˆž,âˆž] be proper convex functions, and let x âˆˆ dom(f1) âˆ© dom(f2).
(a) The following inclusion holds:
âˆ‚f1(x) + âˆ‚f2(x) âŠ† âˆ‚(f1 + f2)(x).
(b) If x âˆˆ int(dom(f1)) âˆ© int(dom(f2)), then
âˆ‚(f1 + f2)(x) = âˆ‚f1(x) + âˆ‚f2(x).

maimization 3.50
The following result shows how to compute the subdifferential set of a maximum of a finite collection of convex functions.
Theorem 3.50 (max rule of subdifferential calculus). Let f1, f2, . . . , fm : E â†’ (âˆ’âˆž, âˆž] be proper convex functions, and define
f(x) = max{f1(x),f2(x),...,fm(x)}. Let x âˆˆ mi=1 int(dom(fi)). Then

âˆ‚f(x) = conv âˆªiâˆˆI(x)âˆ‚fi(x) , where I(x) = {i âˆˆ {1,2,...,m} : fi(x) = f(x)}.

un constrained optimization pronlem fermat s theorem

#### Fenchel conjugates

Fenchelâ€™s inequality
From the definition of conjugate function, we immediately obtain the inequality f(x) + fâˆ—(y) â‰¥ xT y
for all x, y. This is called Fenchelâ€™s inequality (or Youngâ€™s inequality when f is differentiable).
For example with f(x) = (1/2)xT Qx, where Q âˆˆ Sn++, we obtain the inequality xT y â‰¤ (1/2)xT Qx + (1/2)yT Qâˆ’1y.
Conjugate of the conjugate
The examples above, and the name â€˜conjugateâ€™, suggest that the conjugate of the conjugate of a convex function is the original function. This is the case provided a technical condition holds: if f is convex, and f is closed (i.e., epi f is a closed set; see Â§A.3.3), then fâˆ—âˆ— = f. For example, if domf = Rn, then we have fâˆ—âˆ— = f, i.e., the conjugate of the conjugate of f is f again 

Differentiable functions
The conjugate of a differentiable function f is also called the Legendre transform of f. (To distinguish the general definition from the differentiable case, the term Fenchel conjugate is sometimes used instead of conjugate.)
Suppose f is convex and differentiable, with domf = Rn. Any maximizer xâˆ— of yT xâˆ’f(x) satisfies y = âˆ‡f(xâˆ—), and conversely, if xâˆ— satisfies y = âˆ‡f(xâˆ—), then xâˆ— maximizes yT x âˆ’ f(x). Therefore, if y = âˆ‡f(xâˆ—), we have
fâˆ—(y) = xâˆ—T âˆ‡f(xâˆ—) âˆ’ f(xâˆ—).
This allows us to determine fâˆ—(y) for any y for which we can solve the gradient
Then we have fâˆ—(y) = zT âˆ‡f(z) âˆ’ f(z).




Jensen's inequality
let w =sum_i mu_ i w_i and g in partial f(w), we need w in dom f by previous theorem
by def of subgradient:
forall i f(w_ i) >= f(w) + <g, w_ i -w >
therefore sum _ i mu_ i f_i(w) >= sum mu_ i f(w) + sum mu i <g, w i-w> which is 0

example: is log-partition = log sum exp w_ i convex ? proof by jensen or holder s inequalities